{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e0f2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0      1                             Good morning every one\n",
      "1      0  TW: S AssaultActually horrified how many frien...\n",
      "2      1  Thanks by has notice of me Greetings : Jossett...\n",
      "3      0                its ending soon aah unhappy _EMOJI_\n",
      "4      1                         My real time happy _EMOJI_\n"
     ]
    }
   ],
   "source": [
    "# --- 3.0 Data Preparation ---\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib # Library for saving and loading Python objects\n",
    "\n",
    "# Define the correct file path to the processed data\n",
    "# The '../' moves up one directory from the 'notebooks' folder\n",
    "file_path = '../data/processed/combined_data_processed.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to confirm the data is loaded correctly\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e884e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: X=(700,), y=(700,)\n",
      "Validation set shape: X=(150,), y=(150,)\n",
      "Test set shape: X=(150,), y=(150,)\n"
     ]
    }
   ],
   "source": [
    "# Define features (X) and target (y) based on the DataFrame's columns\n",
    "# X is the input data (the text of the posts)\n",
    "# y is the target variable (the sentiment label)\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# Step 1: Split the data into a training set and a temporary set (for validation and testing)\n",
    "# We set test_size=0.3 to allocate 30% of the data for validation and testing.\n",
    "# stratify=y ensures that the proportion of positive and negative labels is the same across all splits.\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split the temporary set in half to create the validation and test sets\n",
    "# We set test_size=0.5 to split the 30% temporary set evenly into two 15% sets.\n",
    "# stratify=y_temp ensures the label proportions are maintained in these final splits.\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Display the shapes of the splits to confirm the sizes and successful splitting\n",
    "print(f\"Training set shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Validation set shape: X={X_val.shape}, y={y_val.shape}\")\n",
    "print(f\"Test set shape: X={X_test.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54c16185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized Training set shape: (700, 1000)\n",
      "Vectorized Validation set shape: (150, 1000)\n",
      "Vectorized Test set shape: (150, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TF-IDF Vectorizer\n",
    "# This converts the text data into a matrix of numerical features.\n",
    "# max_features=1000 limits the vocabulary to the 1000 most common words, reducing complexity.\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "# Fit the vectorizer on the training data and transform it\n",
    "# We only fit on the training data to prevent data leakage from the validation and test sets.\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the validation and test data using the fitted vectorizer\n",
    "# We use the same vocabulary and scaling learned from the training data.\n",
    "X_val_vec = vectorizer.transform(X_val)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Display the final shapes of the vectorized data\n",
    "print(f\"Vectorized Training set shape: {X_train_vec.shape}\")\n",
    "print(f\"Vectorized Validation set shape: {X_val_vec.shape}\")\n",
    "print(f\"Vectorized Test set shape: {X_test_vec.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d7c63e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prepared data and vectorizer saved to '../models/prepared_data.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Define the file path for saving\n",
    "# The '../' moves up one directory from the 'notebooks' folder\n",
    "save_path = '../models/prepared_data.pkl'\n",
    "\n",
    "# Define a dictionary to store all the prepared data\n",
    "prepared_data = {\n",
    "    'X_train_vec': X_train_vec,\n",
    "    'X_val_vec': X_val_vec,\n",
    "    'X_test_vec': X_test_vec,\n",
    "    'y_train': y_train,\n",
    "    'y_val': y_val,\n",
    "    'y_test': y_test,\n",
    "    'vectorizer': vectorizer\n",
    "}\n",
    "\n",
    "# Save the dictionary of prepared data to a single file\n",
    "joblib.dump(prepared_data, save_path)\n",
    "print(f\"\\nPrepared data and vectorizer saved to '{save_path}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emoji-sentiment-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "618548a3",
   "metadata": {},
   "source": [
    "# üìò 1.5 Data Cleaning and Validation\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "This notebook performs **deterministic cleaning and validation** of the raw emoji sentiment datasets identified in Notebook 1.0.\n",
    "\n",
    "The goal is to produce **analysis-ready datasets** while preserving the original semantic content of the data, particularly emoji usage in text.\n",
    "\n",
    "No feature engineering, modeling, or exploratory analysis is performed here.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f4a755",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "* Remove redundant and structural artifacts from raw datasets\n",
    "* Standardize column naming and data types\n",
    "* Preserve emojis exactly as they appear in raw text\n",
    "* Produce validated, reusable datasets for downstream analysis\n",
    "* Establish explicit dataset contracts for future notebooks\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82a4a8",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "This notebook consumes the following raw datasets:\n",
    "\n",
    "* `1k_data_emoji_tweets_senti_posneg.csv`\n",
    "  Labeled tweet text containing emojis and sentiment labels\n",
    "\n",
    "* `15_emoticon_data.csv`\n",
    "  A small emoji reference table containing Unicode metadata\n",
    "\n",
    "These files are treated as **read-only inputs**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8149e1b5",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "\n",
    "This notebook produces the following cleaned datasets:\n",
    "\n",
    "* `tweets_clean.csv`\n",
    "  Canonical tweet dataset for modeling and analysis\n",
    "\n",
    "* `emoji_reference_clean.csv`\n",
    "  Canonical emoji lookup table for optional downstream use\n",
    "\n",
    "All outputs are saved to `data/processed/`.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd87624",
   "metadata": {},
   "source": [
    "## üß© Section 1: Setup and Imports\n",
    "\n",
    "This section defines the runtime environment, logging configuration, and filesystem paths used throughout the notebook.\n",
    "\n",
    "The goal is to make all subsequent steps deterministic and reproducible.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c15ce4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1.5 Data Cleaning and Validation ---\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Paths\n",
    "RAW_DATA_DIR = Path(\"data/raw\")\n",
    "PROCESSED_DATA_DIR = Path(\"data/processed\")\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47908295",
   "metadata": {},
   "source": [
    "## üß© Section 2: Load Raw Datasets\n",
    "\n",
    "Raw datasets are loaded from disk without modification.\n",
    "\n",
    "At this stage:\n",
    "\n",
    "* No cleaning is applied\n",
    "* No assumptions are made\n",
    "* Data is treated strictly as source material\n",
    "\n",
    "This step exists to clearly separate **data ingestion** from **data transformation**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8df706a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading raw datasets...\n",
      "INFO:__main__:Datasets loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Loading raw datasets...\")\n",
    "\n",
    "tweets_raw = pd.read_csv(RAW_DATA_DIR / \"1k_data_emoji_tweets_senti_posneg.csv\")\n",
    "emoji_raw = pd.read_csv(RAW_DATA_DIR / \"15_emoticon_data.csv\")\n",
    "\n",
    "logger.info(\"Datasets loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932e9d91",
   "metadata": {},
   "source": [
    "## üßπ Section 3: Clean Tweet Sentiment Dataset\n",
    "\n",
    "This section cleans the primary modeling dataset containing tweet text and sentiment labels.\n",
    "\n",
    "### Cleaning decisions applied:\n",
    "\n",
    "* Remove redundant index columns\n",
    "* Rename columns to standardized names (`text`, `label`)\n",
    "* Enforce correct data types\n",
    "* Remove rows with missing critical values\n",
    "* Preserve emojis and raw text exactly\n",
    "\n",
    "No semantic transformations are performed.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e822e670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Cleaned tweets dataset shape: (1000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Good morning every one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>TW: S AssaultActually horrified how many frien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Thanks by has notice of me Greetings : Jossett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>its ending soon aah unhappy üòß</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>My real time happy üòä</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1                             Good morning every one\n",
       "1      0  TW: S AssaultActually horrified how many frien...\n",
       "2      1  Thanks by has notice of me Greetings : Jossett...\n",
       "3      0                      its ending soon aah unhappy üòß\n",
       "4      1                               My real time happy üòä"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = tweets_raw.copy()\n",
    "\n",
    "# Drop redundant index column\n",
    "tweets = tweets.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    "\n",
    "# Rename columns\n",
    "tweets = tweets.rename(\n",
    "    columns={\n",
    "        \"post\": \"text\",\n",
    "        \"sentiment\": \"label\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Enforce data types\n",
    "tweets[\"text\"] = tweets[\"text\"].astype(str)\n",
    "tweets[\"label\"] = tweets[\"label\"].astype(int)\n",
    "\n",
    "# Drop rows with missing values in critical columns\n",
    "tweets = tweets.dropna(subset=[\"text\", \"label\"])\n",
    "\n",
    "# Reset index\n",
    "tweets = tweets.reset_index(drop=True)\n",
    "\n",
    "logger.info(f\"Cleaned tweets dataset shape: {tweets.shape}\")\n",
    "tweets.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea40593",
   "metadata": {},
   "source": [
    "## üßπ Section 4: Clean Emoji Reference Dataset\n",
    "\n",
    "This section cleans the emoji reference dataset used as a lookup table.\n",
    "\n",
    "### Cleaning decisions applied:\n",
    "\n",
    "* Remove redundant index columns\n",
    "* Standardize column names using snake_case\n",
    "* Preserve Unicode codepoints and names exactly\n",
    "* Treat the dataset as non-modeling metadata\n",
    "\n",
    "No sentiment labels are inferred or assigned.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae1e4639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Cleaned emoji reference dataset shape: (16, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emoji</th>\n",
       "      <th>unicode_codepoint</th>\n",
       "      <th>unicode_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>üòç</td>\n",
       "      <td>0x1f60d</td>\n",
       "      <td>SMILING FACE WITH HEART-SHAPED EYES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>üò≠</td>\n",
       "      <td>0x1f62d</td>\n",
       "      <td>LOUDLY CRYING FACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>üòò</td>\n",
       "      <td>0x1f618</td>\n",
       "      <td>FACE THROWING A KISS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>üòä</td>\n",
       "      <td>0x1f60a</td>\n",
       "      <td>SMILING FACE WITH SMILING EYES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>üòÅ</td>\n",
       "      <td>0x1f601</td>\n",
       "      <td>GRINNING FACE WITH SMILING EYES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  emoji unicode_codepoint                         unicode_name\n",
       "0     üòç           0x1f60d  SMILING FACE WITH HEART-SHAPED EYES\n",
       "1     üò≠           0x1f62d                   LOUDLY CRYING FACE\n",
       "2     üòò           0x1f618                 FACE THROWING A KISS\n",
       "3     üòä           0x1f60a       SMILING FACE WITH SMILING EYES\n",
       "4     üòÅ           0x1f601      GRINNING FACE WITH SMILING EYES"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_ref = emoji_raw.copy()\n",
    "\n",
    "# Drop redundant index column\n",
    "emoji_ref = emoji_ref.drop(columns=[\"Unnamed: 0\"], errors=\"ignore\")\n",
    "\n",
    "# Rename columns to snake_case\n",
    "emoji_ref = emoji_ref.rename(\n",
    "    columns={\n",
    "        \"Emoji\": \"emoji\",\n",
    "        \"Unicode codepoint\": \"unicode_codepoint\",\n",
    "        \"Unicode name\": \"unicode_name\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Enforce string types\n",
    "for col in emoji_ref.columns:\n",
    "    emoji_ref[col] = emoji_ref[col].astype(str)\n",
    "\n",
    "# Reset index\n",
    "emoji_ref = emoji_ref.reset_index(drop=True)\n",
    "\n",
    "logger.info(f\"Cleaned emoji reference dataset shape: {emoji_ref.shape}\")\n",
    "emoji_ref.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac125e9",
   "metadata": {},
   "source": [
    "## ‚úÖ Section 5: Validation Checks\n",
    "\n",
    "This section enforces **hard invariants** that downstream notebooks may rely on.\n",
    "\n",
    "Validation includes:\n",
    "\n",
    "* Schema checks\n",
    "* Value range checks\n",
    "* Non-null constraints\n",
    "* Uniqueness guarantees\n",
    "\n",
    "If any validation fails, the pipeline should stop immediately.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "864e6bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:All validation checks passed.\n"
     ]
    }
   ],
   "source": [
    "# Tweet dataset checks\n",
    "assert tweets.columns.tolist() == [\"label\", \"text\"]\n",
    "assert tweets[\"label\"].isin([0, 1]).all()\n",
    "assert tweets[\"text\"].str.len().gt(0).all()\n",
    "\n",
    "# Emoji reference checks\n",
    "assert \"emoji\" in emoji_ref.columns\n",
    "assert emoji_ref[\"emoji\"].nunique() == len(emoji_ref)\n",
    "\n",
    "logger.info(\"All validation checks passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc2ad26",
   "metadata": {},
   "source": [
    "## üíæ Section 6: Persist Cleaned Datasets\n",
    "\n",
    "Cleaned datasets are written to disk in a stable, reusable format.\n",
    "\n",
    "From this point forward:\n",
    "\n",
    "* Downstream notebooks must read from `data/processed/`\n",
    "* Raw datasets should no longer be accessed directly\n",
    "\n",
    "This establishes a clear boundary between data preparation and analysis.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0b1a570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Saved cleaned tweets to data\\processed\\tweets_clean.csv\n",
      "INFO:__main__:Saved emoji reference to data\\processed\\emoji_reference_clean.csv\n"
     ]
    }
   ],
   "source": [
    "tweets_out = PROCESSED_DATA_DIR / \"tweets_clean.csv\"\n",
    "emoji_out = PROCESSED_DATA_DIR / \"emoji_reference_clean.csv\"\n",
    "\n",
    "tweets.to_csv(tweets_out, index=False)\n",
    "emoji_ref.to_csv(emoji_out, index=False)\n",
    "\n",
    "logger.info(f\"Saved cleaned tweets to {tweets_out}\")\n",
    "logger.info(f\"Saved emoji reference to {emoji_out}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fc7d6e",
   "metadata": {},
   "source": [
    "## üìå Section 7: Dataset Contracts\n",
    "\n",
    "### `tweets_clean.csv`\n",
    "\n",
    "| Column  | Description                                         |\n",
    "| ------- | --------------------------------------------------- |\n",
    "| `text`  | Raw tweet text with emojis preserved                |\n",
    "| `label` | Binary sentiment label (0 = negative, 1 = positive) |\n",
    "\n",
    "### `emoji_reference_clean.csv`\n",
    "\n",
    "| Column              | Description                |\n",
    "| ------------------- | -------------------------- |\n",
    "| `emoji`             | Unicode emoji character    |\n",
    "| `unicode_codepoint` | Official Unicode codepoint |\n",
    "| `unicode_name`      | Official Unicode name      |\n",
    "\n",
    "‚ö†Ô∏è No sentiment information is encoded in the emoji reference dataset.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304fc8e0",
   "metadata": {},
   "source": [
    "## üîí Scope and Guarantees\n",
    "\n",
    "This notebook guarantees that:\n",
    "\n",
    "* Emojis are preserved and not collapsed\n",
    "* No modeling assumptions are introduced\n",
    "* Cleaning steps are deterministic and auditable\n",
    "\n",
    "This notebook **does not**:\n",
    "\n",
    "* Engineer features\n",
    "* Define targets beyond existing labels\n",
    "* Perform exploratory analysis\n",
    "* Train models\n",
    "\n",
    "Those steps belong in subsequent notebooks.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ceace4",
   "metadata": {},
   "source": [
    "## ‚û°Ô∏è Next Steps\n",
    "\n",
    "The cleaned datasets produced here enable multiple downstream paths, including:\n",
    "\n",
    "* Text-only sentiment modeling\n",
    "* Emoji-aware feature augmentation\n",
    "* Lexicon-based emoji sentiment analysis\n",
    "\n",
    "These decisions are intentionally deferred to Notebook 2.0.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
